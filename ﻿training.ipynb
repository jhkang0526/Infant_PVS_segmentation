{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f899d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import monai\n",
    "import nibabel as nib\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "from monai.transforms import *\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "from skimage import measure\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481869d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BG seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85288f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader passed!\n",
    "# Using a custom data path and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = monai.networks.nets.BasicUNet(spatial_dims=3, \n",
    "                                      in_channels=1,\n",
    "                                      out_channels=2,\n",
    "                                      features=(32, 32, 64, 128, 256, 32),\n",
    "                                     )\n",
    "device = \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0067c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\"\"\"\n",
    "lossfunction.\n",
    "\n",
    "References\n",
    "----------\n",
    "Gros C et al., MedIA (2021). DOI: https://doi.org/10.1016/j.media.2021.102038\n",
    "\"\"\"\n",
    "\n",
    "class AWing(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=2.1, omega=8, epsilon=1, theta=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha   = float(alpha)\n",
    "        self.omega   = float(omega)\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.theta   = float(theta)\n",
    "\n",
    "    def forward(self, y_pred , y):\n",
    "        lossMat = torch.zeros_like(y_pred)\n",
    "        A = self.omega * (1/(1+(self.theta/self.epsilon)**(self.alpha-y)))*(self.alpha-y)*((self.theta/self.epsilon)**(self.alpha-y-1))/self.epsilon\n",
    "        C = self.theta*A - self.omega*torch.log(1+(self.theta/self.epsilon)**(self.alpha-y))\n",
    "        case1_ind = torch.abs(y-y_pred) < self.theta\n",
    "        case2_ind = torch.abs(y-y_pred) >= self.theta\n",
    "        lossMat[case1_ind] = self.omega*torch.log(1+torch.abs((y[case1_ind]-y_pred[case1_ind])/self.epsilon)**(self.alpha-y[case1_ind]))\n",
    "        lossMat[case2_ind] = A[case2_ind]*torch.abs(y[case2_ind]-y_pred[case2_ind]) - C[case2_ind]\n",
    "        return lossMat\n",
    "\n",
    "class Loss_weighted(nn.Module):\n",
    "    def __init__(self, W=10, alpha=2.1, omega=8, epsilon=1, theta=0.5):\n",
    "        super().__init__()\n",
    "        self.W = float(W)\n",
    "        self.Awing = AWing(alpha, omega, epsilon, theta)\n",
    "\n",
    "    def forward(self, y_pred, y, M):\n",
    "        M = M.float()\n",
    "        Loss = self.Awing(y_pred,y)\n",
    "        weighted = Loss * (self.W * M + 1.)\n",
    "        return weighted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = Loss_weighted(W=10)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27830db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU = torch.nn.ReLU()\n",
    "\n",
    "def normReLU(input):\n",
    "    if torch.max(ReLU(input)) != 0:\n",
    "        output = (ReLU(input)) / (torch.max(ReLU(input)))\n",
    "    else: output = torch.zeros(input.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953eb528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    dice_vals = list()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(epoch_iterator_val):\n",
    "            val_inputs, val_labels = (batch[\"t2\"]).to(device), batch[\"seg\"].to(device)\n",
    "            # val_labels[val_labels > 0] = 1\n",
    "            \n",
    "            val_outputs = model(val_inputs)\n",
    "            for b in range(val_outputs.shape[0]):\n",
    "                for c in range(val_outputs.shape[1]):\n",
    "                    val_outputs[b,c] = normReLU(val_outputs[b,c])\n",
    "\n",
    "            val_outputs_bimask = (val_outputs[:,1] > 0.42).unsqueeze(1)\n",
    "            \n",
    "            if step==0:\n",
    "                plt.figure(dpi=256)\n",
    "                plt.subplot(231)\n",
    "                plt.imshow(torch.sum(val_inputs[0,0].cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('input')\n",
    "                plt.subplot(232)\n",
    "                plt.imshow(torch.sum(val_outputs[0,1].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('output')\n",
    "                plt.subplot(233)\n",
    "                plt.imshow(torch.sum(val_labels[0,0].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('label')\n",
    "                plt.subplot(234)\n",
    "                plt.imshow(torch.sum(val_inputs[1,0].cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('input')\n",
    "                plt.subplot(235)\n",
    "                plt.imshow(torch.sum(val_outputs[1,1].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('output')\n",
    "                plt.subplot(236)\n",
    "                plt.imshow(torch.sum(val_labels[1,0].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('label')\n",
    "                plt.savefig(os.path.join(log_dir, 'validation', f'valid_step{global_step}.png'))\n",
    "                plt.show()         \n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [\n",
    "                post_label(val_label_tensor) for val_label_tensor in val_labels_list\n",
    "            ]\n",
    "            val_outputs_list = decollate_batch(val_outputs_bimask)\n",
    "            val_output_convert = [\n",
    "                post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list\n",
    "            ]\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            dice = dice_metric.aggregate().item()\n",
    "            dice_vals.append(dice)\n",
    "            epoch_iterator_val.set_description(\n",
    "                \"Validate (%d / %d Steps) (dice=%2.5f)\" % (global_step, 10.0, dice)\n",
    "            )\n",
    "                  \n",
    "        dice_metric.reset()\n",
    "    mean_dice_val = np.mean(dice_vals)\n",
    "    return mean_dice_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(\n",
    "        train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True\n",
    "    )\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "        x, y = (batch[\"t2\"]).to(device), batch[\"seg\"].to(device)\n",
    "        \n",
    "        mask = y.clone()\n",
    "        mask[mask>0] = 1\n",
    "    \n",
    "        logit_map = model(x)\n",
    "        \n",
    "        for b in range(logit_map.shape[0]):\n",
    "            for c in range(logit_map.shape[1]):\n",
    "                logit_map[b,c] = normReLU(logit_map[b,c])\n",
    "\n",
    "        # loss = (loss_function(logit_map[:,1:], y)).mean()\n",
    "        loss = loss_function(logit_map[:,1:], y, mask)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_iterator.set_description(\n",
    "            \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss)\n",
    "        )\n",
    "        if (\n",
    "            global_step % eval_num == 0 and global_step != 0\n",
    "        ) or global_step == max_iterations:\n",
    "            epoch_iterator_val = tqdm(\n",
    "                valid_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True\n",
    "            )\n",
    "            dice_val = validation(epoch_iterator_val)\n",
    "            metric_values.append(dice_val)\n",
    "            \n",
    "            plt.figure(1, figsize=(12,8))\n",
    "            plt.plot(metric_values)\n",
    "            plt.xlabel(f\"{eval_num}\")\n",
    "            plt.savefig(os.path.join(log_dir, \"Dice_plot.png\"))\n",
    "            plt.close(1)\n",
    "            \n",
    "            if dice_val > dice_val_best:\n",
    "                dice_val_best = dice_val\n",
    "                global_step_best = global_step\n",
    "                torch.save(\n",
    "                    model.state_dict(), os.path.join(log_dir, \"best_model.pth\")\n",
    "                )\n",
    "                print(\n",
    "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "    \n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step%100==0:        \n",
    "            scheduler.step()\n",
    "    \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "    plt.figure(1, figsize=(12, 8))\n",
    "    plt.plot(epoch_loss_values)\n",
    "    plt.savefig(os.path.join(log_dir,\"loss.png\"))\n",
    "    plt.close(1)\n",
    "    \n",
    "    file = open(f'{log_dir}/train.txt', 'a')\n",
    "    file.write(f'global_step : {str(global_step)}\\n')\n",
    "    file.write('current dice best : ')\n",
    "    file.write(str(dice_val_best)+'\\n')\n",
    "        \n",
    "\n",
    "\n",
    "    return global_step, dice_val_best, global_step_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 6000\n",
    "eval_num = 64\n",
    "post_label = AsDiscrete(to_onehot=2)\n",
    "post_pred = AsDiscrete(argmax=False, to_onehot=2)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84577cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                              lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=True)\n",
    "\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(\n",
    "        global_step, train_loader, dice_val_best, global_step_best\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294c3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433348b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PVS seg\n",
    "# using training label after correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9380d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import monai\n",
    "import nibabel as nib\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "from monai.transforms import *\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "from skimage import measure\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b01380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader passed!\n",
    "# Using a custom data path and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = monai.networks.nets.BasicUNet(spatial_dims=3, \n",
    "                                      in_channels=1,\n",
    "                                      out_channels=2,\n",
    "                                      features=(32, 32, 64, 128, 256, 32),\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ab7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= \"cuda:0\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    step = 0\n",
    "    dice_vals = list()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(epoch_iterator_val):\n",
    "            step += 1\n",
    "            val_inputs, val_labels =  batch[\"t2\"].to(device), batch[\"seg\"].to(device)\n",
    "            val_labels[val_labels>0]=1\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_outputs_bimask = (val_outputs[:,1] > 0.42).unsqueeze(1)\n",
    "            \n",
    "            if step==0:\n",
    "                plt.figure(dpi=256)\n",
    "                plt.subplot(231)\n",
    "                plt.imshow(torch.sum(val_inputs[0,0].cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('input')\n",
    "                plt.subplot(232)\n",
    "                plt.imshow(torch.sum(val_outputs[0,1].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('output')\n",
    "                plt.subplot(233)\n",
    "                plt.imshow(torch.sum(val_labels[0,0].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('label')\n",
    "                plt.subplot(234)\n",
    "                plt.imshow(torch.sum(val_inputs[1,0].cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('input')\n",
    "                plt.subplot(235)\n",
    "                plt.imshow(torch.sum(val_outputs[1,1].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('output')\n",
    "                plt.subplot(236)\n",
    "                plt.imshow(torch.sum(val_labels[1,0].detach().cpu(),axis=2),cmap='gray')\n",
    "                plt.axis('off')\n",
    "                plt.title('label')\n",
    "                plt.savefig(os.path.join(log_dir, 'validation', f'valid_step{global_step}.png'))\n",
    "                plt.show()      \n",
    "                \n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [\n",
    "                post_label(val_label_tensor) for val_label_tensor in val_labels_list\n",
    "            ]\n",
    "            val_outputs_list = decollate_batch(val_outputs_bimask)\n",
    "            val_output_convert = [\n",
    "                post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list\n",
    "            ]\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            dice = dice_metric.aggregate().item()\n",
    "            dice_vals.append(dice)\n",
    "            epoch_iterator_val.set_description(\n",
    "                \"Validate (%d / %d Steps) (dice=%2.5f)\" % (global_step, 10.0, dice)\n",
    "            )\n",
    "                  \n",
    "        dice_metric.reset()\n",
    "    mean_dice_val = np.mean(dice_vals)\n",
    "    return mean_dice_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(\n",
    "        train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True\n",
    "    )\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "        t2, seg = batch[\"t2\"].to(device), batch[\"seg\"].to(device)\n",
    "        seg[seg>0]=1\n",
    "        \n",
    "        pred = model(t2)\n",
    "        loss = loss_function(pred, seg)\n",
    "        \n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_iterator.set_description(\n",
    "            \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss)\n",
    "        )\n",
    "        if (\n",
    "            global_step % eval_num == 0 and global_step != 0\n",
    "        ) or global_step == max_iterations:\n",
    "            epoch_iterator_val = tqdm(\n",
    "                valid_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True\n",
    "            )\n",
    "            dice_val = validation(epoch_iterator_val)\n",
    "            metric_values.append(dice_val)\n",
    "            \n",
    "            plt.figure(1, figsize=(12,8))\n",
    "            plt.plot(metric_values)\n",
    "            plt.xlabel(f\"{eval_num}\")\n",
    "            plt.savefig(os.path.join(log_dir, \"dice_plot.png\"))\n",
    "            plt.close(1)\n",
    "            \n",
    "            if dice_val > dice_val_best:\n",
    "                dice_val_best = dice_val\n",
    "                global_step_best = global_step\n",
    "                torch.save(\n",
    "                    model.state_dict(), os.path.join(log_dir, \"model\", \"best_model.pth\")\n",
    "                )\n",
    "                print(\n",
    "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                    \n",
    "                )\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step%160==0:        \n",
    "            scheduler.step()\n",
    "    \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "    plt.figure(1, figsize=(12, 8))\n",
    "    plt.plot(epoch_loss_values)\n",
    "    plt.savefig(os.path.join(log_dir,\"loss.png\"))\n",
    "    plt.close(1)\n",
    "    \n",
    "    file = open(f'{log_dir}/train.txt', 'a')\n",
    "    file.write(f'global_step : {str(global_step)}\\n')\n",
    "    file.write('current dice best : ')\n",
    "    file.write(str(dice_val_best)+'\\n')\n",
    "    \n",
    "    return global_step, dice_val_best, global_step_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 6000\n",
    "eval_num = 64\n",
    "post_label = AsDiscrete(to_onehot=2)\n",
    "post_pred = AsDiscrete(argmax=False, to_onehot=2)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146414dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                              lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=True)\n",
    "\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(\n",
    "        global_step, train_loader, dice_val_best, global_step_best\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
